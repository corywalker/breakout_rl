\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{hyperref}

%\usetikzlibrary{automata,positioning}
\usetikzlibrary{shapes, arrows}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\rhead{\hmwkClass\ (\hmwkClassInstructor)}
\cfoot{\thepage}

\lstset{
  caption=\lstname,
  %backgroundcolor=\color{lightgray},
  literate={\$}{{\$}}1,
  breaklines=true,
  basicstyle=\ttfamily\small
}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\hmwkTitle}{Utilizing reinforcement learning techniques to play Atari's Breakout}
\newcommand{\hmwkDueDate}{November 3, 2015}
\newcommand{\hmwkClass}{Reinforcement Learning}
\newcommand{\hmwkClassInstructor}{Dr. Itamar Arel}
\newcommand{\hmwkAuthorName}{Andrew Messing, Ben Brock, and Cory Walker}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Alias for the Solution section header
\newcommand{\solution}{ \hfill \break \break \textbf{Solution} \hfill \break \break}

\begin{document}

\maketitle

\pagebreak

\begin{abstract}
  Abstract goes here. Ben, could you take care of this?
\end{abstract}

\section{Introduction}
Deep Reinforment Learning is a relatively new approach to learn to control agents via high dimensional sensory inputs like vision in speech. This project compares success of a specific feature based reinforcement learning approach to a deep reinforcement learning one using the atari game breakout and the Arcade Learning Environment.

\section{Breakout and feature extraction}
\subsection{Breakout}
Breakout is a classic arcade game developed by Atari, Inc. Similar to pong the object is to use a paddle to bounce a ball; however for breakout the paddle move horizontally and is positioned at the bottom of the screen. At the top of the screen are multiple layers of bricks. Everytime the ball collides with a brick it breaks it and the player gains points. The goal of the game is to break all the bricks, score points, and not let the ball fall below the paddle as that costs the user a life. When all the lives are gone the game is over.

  \begin{center}
  \includegraphics[width=50mm]{tmp.jpg}
  \end{center}

\subsection{Arcade Learning Environment}
The Arcade Learning Environment (ALE) is a simple object-oriented framework designed to allow researchers to test Artificial Intelligence agents to play Atari 2600 games \cite{ale}. The agent can receive the game frames and ram and is given access to the number of lives, the score, and other information from the game. Then the agent can use this to come with an action that it would send back to the ALE. The controller that is simulated contains a joystick, an action button, and a couple of menu buttons that total to about 20 possible actions including the buttons being pressed and the joystick being moved in one of eight directions.

For breakout specifically there are three actions that contribute to the game: move the joystick left, move the joystick right, and do nothing.

\subsection{Feature Extraction}
To extract features for the non-Deep reinforcement learning agent image processing was used to capture the paddle and the ball's position. While the paddle maintained one of two colors making it relatively easy to extract, the ball changes colors as it goes by the brick layers to the color of that brick layer and when it is in open space to the color of the paddle. 

  \begin{center}
  \includegraphics[scale=.75]{tmp2.jpg}
  \end{center}

\section{Reinforcement learning}
Ben's section goes here. Be sure to cite Sutton at some point \cite{sutton}.

\section{Deep reinforcement learning}
  While manual feature extraction can result in faster training and potentially better performance, there is a large motivation to be able to learn from general and high dimensional inputs such as video and sound. If we could learn to play Atari games from raw pixels, we could use the same algorithm on an array of games, even games we have never actually played before. \\

  Of course, things are not as simple as creating a state for every possible screen. Since the Atari environment has 128 colors and a 210x160 screen, the number of states resulting from this would be $128^{210 \cdot 160} \approx 1.799\times 10^{70802}$. Even if we reduced the number of colors to 4, we would still have a number with 20,000 digits for the number of states. Furthermore, even if we cut the screen size by a quarter, we would still have $4^{105 \cdot 80} \approx 2.013\times 10^{5057}$ states. \\

  On top of all this, just a single screen in Breakout and many other Atari games is just a partial representation of the game state. The ball could be moving either right or left, which is an important piece of information for scoring points. \\

  Recently Google DeepMind has made progress in this problem space by utilizing convolutional neural networks for estimating the action-value function based on visual input \cite{dmnips}. The rest of this project will describe implementing DeepMind's Double DQN algorithm, an algorithm for which no open-source implementation previously existed.
\subsection{Deep Q network}
  At its core, the Deep Q network is a function approximator for the action-value function. The standard Deep Q network for playing Atari games features around 1.5 million parameters. Each parameter is updated through the following equation:
  \[ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t+\alpha(Y_t^Q-Q(S_t,A_t;\boldsymbol{\theta}_t))\nabla_{\boldsymbol{\theta}_t}Q(S_t,A_t;\boldsymbol{\theta}_t) \]
  The network is composed of the following stacked sequence of layers:
  \begin{enumerate}
    \item An input layer for the past 4 scaled (84x84) and grayscale snapshots of the screen.
    \item A 2D convolution layer with 32 filters of size 8 and stride 4. The layer uses a ReLU nonlinearity.
    \item A 2D convolution layer with 64 filters of size 4 and stride 2. The layer uses a ReLU nonlinearity.
    \item A 2D convolution layer with 64 filters of size 3 and stride 1. The layer uses a ReLU nonlinearity.
    \item Next we have a fully-connected layer with 512 units. This layer uses a ReLU nonlinearity.
    \item Finally we have another fully-connected layer as the output layer with $|A|$ outputs. This layer uses linear activation.
  \end{enumerate}

\subsection{DQN variations}
  DeepMind first presented their version of the Deep Q network at the NIPS Deep Learning Workshop in late 2013. In this paper they train the network on seven Atari games, surpassing a human expert on three of them. The network described in this paper requires roughly 24 hours of training for a game on a standard GPU.\\

  In early 2015, DeepMind published a paper in Nature which updates the network structure originally defined. The new network is similar to the NIPS one, but it features much more conservative hyperparameters (increasing training time to a week) and first describes model freezing. \\

Model freezing is a technique that separates the action-value function into two functions with the same structure but different weights. The main action-value function has weights $\boldsymbol{\theta}_t$ and the target action-value function has weights $\boldsymbol{\theta}_t^-$. Every 10,000 training updates we set $\boldsymbol{\theta}_t^- \leftarrow \boldsymbol{\theta}_t$ to keep the target action-value function in sync with the main one. It is possible that the NIPS version used model freezing as well, but the paper made no mention of it and therefore the implementations of the NIPS-described DQN do not feature model freezing. \\

In September 2015, DeepMind updated the algorithm described in Nature to address value overestimates. They call this algorithm Double DQN, and this is the algorithm that we will implement and apply to Atari Breakout.

\subsection{Double DQN formulation}
  The motivation behind Double Q-learning is that max operator uses the same values for action selection and evaluation, which can cause an issue with overoptimistic value estimates, especially with function approximation. If all action-values are uniformly overestimated, this would not affect the policy. The authors of the Double DQN paper argue that the overestimations are not uniform, leading to suboptimal policies. Double Q learning attempts to address these overestimations by decoupling action selection from action evaluation. While traditional Double Q-learning involves training two different value functions, van Hasselt et. al. argue that the same effect can be achieved for DQN with the following change to the target function $Y_t$:

  \[Y_t^{\textrm{DQN}} \equiv R_{t+1} + \gamma \textcolor{red}{\max_{a} Q(S_{t+1},a;\boldsymbol{\theta}_t^-)} \]
  \[ \Downarrow \]
  \[\boxed{Y_t^{\textrm{DoubleDQN}} \equiv R_{t+1} + \gamma \textcolor{red}{Q(S_{t+1},\argmax_a Q(S_{t+1},a;\boldsymbol{\theta}_t),\boldsymbol{\theta}_t^-)}} \]

  We can reason that if $\boldsymbol{\theta}_t^- = \boldsymbol{\theta}_t$, there is no difference between the two equations. This is why Double DQN requires model freezing to work correctly. Since we use the target function $Y_t$ to calcuate the approximation errors with which to train our network, changing this function changes the policies that our network learns. According to the Double DQN authors, these different policies led to higher scores in many games.

\subsection{Implementation}
  We chose to implement Double DQN both because we are interested in the algorithm and also because there were not previously any open source implementations of Double DQN. Only the paper existed. We built on a very good existing open source implementation for vanilla DQN built by Nathan Sprague, \href{https://github.com/spragunr/deep_q_rl}{deep\_q\_rl}. This implementation uses a Lasagne/Theano backend with useful code for model import/export and visualization of the gameplay. It also features an efficient implementation of the replay buffer that uses a circular buffer to store the 1M replay frames. Furthermore, Sprague's implementation already supported the NIPS and Nature algorithms, giving us a baseline with which to compare our results from Double DQN. \\

We performed our testing on an EC2 g2.2xlarge instance equipped with a single NVIDIA Kepler GK104 GPU and 15GB of onboard memory. Our single-threaded program was CPU bound and only used 30\% of the GPU, so it was possible to train another network simultaneously, as long as we reduced the replay buffer size from 8GB to 7GB. One main issue with testing these networks is that they can take up to 12 hours to determine if there is an issue with learning. After a few iterations of making changes and testing them, we arrived with a solution that stayed true to the Double DQN paper and seemed to learn at a good rate. \\

Our open source implementation of Double DQN can be viewed and commented on by visiting the pull request at \href{https://github.com/spragunr/deep_q_rl/pull/52}{https://github.com/spragunr/deep\_q\_rl/pull/52}. The fork with the implementation is located at \href{https://github.com/corywalker/deep_q_rl}{https://github.com/corywalker/deep\_q\_rl}.

\subsection{Results}
We trained both a DQN and a Double DQN on Atari Breakout for about 20 hours:
  \begin{figure}[H]
    \centering
    \includegraphics[width=120mm]{dqn_rewardper.pdf}
  \end{figure}
  Each epoch takes about an hour, and we only trained for a few epochs out of the recommended 200. The performance comparison between DQN and Double DQN for Breakout is inconclusive for Breakout after just 20 epochs. This should be expected since the Double DQN paper reported roughly equivalent performance for Breakout. Ideally, we should have picked a game with more pronounced difference for Double DQN, such as Space Invaders. The number of training steps per epoch is 250000, and we skip every 4 frames, meaning that each epoch is actually 1M frames. Since the ALE is 60fps, each epoch is equivalent to ~278 minutes of gameplay. The DeepMind paper quoted Breakout human performance at 30 points per episode. Since it seems to take 6 epochs to reach this (in both Double DQN and DQN), we estimate that this is $6 \cdot 278 = 1668 \mbox{ mins} = 27.8 \mbox{ hours}$ of gameplay to reach human level performance. In terms of computation time, it takes about an hour or two. \\

  The plots of the mean value estimates indicate that Double DQN accomplishes its purpose of mitigating value overestimation:
  \begin{figure}[H]
    \centering
    \includegraphics[width=120mm]{dqn_meanq.pdf}
  \end{figure}

\section{Design challenges}
The computational burden of Deep Q learning can be cost prohibitive on a small budget. The amount of computation also increases development turnaround time, since often only one set of changes can be tried in a day. \\

Put more design challenges here.

\section{Summary}
A summary goes here.


\begin{thebibliography}{1}
\bibitem{ale} Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. {\em The Arcade Learning Environment: An Evaluation Platform for General Agents}. In Journal of Artificial Intelligence Research 47, pp. 253-279, 2013.
\bibitem{sutton} R. S. Sutton and A. G. Barto, {\em Reinforcement Learning: An Introduction}. Cambridge, MA: MIT Press, 1998.
\bibitem{dmnips} Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., \& Riedmiller, M. (2013). Playing Atari with deep reinforcement learning. \textit{arXiv preprint arXiv:1312.5602}.
\bibitem{dmdoubl} van Hasselt, H., Guez, A., \& Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. \textit{arXiv preprint arXiv:1509.06461}.
\end{thebibliography}

%\section{Appendix}
%\begin{center}
  %\lstinputlisting{/Users/cwalker32/Documents/LaTeX/ece_517/proj2/et.m}
%\end{center}


\pagebreak

\end{document}
